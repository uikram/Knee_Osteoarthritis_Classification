{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45a28da3",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT) — Knee OA Classifier\n",
    "*Generated on 2025-09-23 14:43:50*\n",
    "\n",
    "Trains a **Vision Transformer (ViT_B_16)** on the Knee OA dataset with:\n",
    "- Same augmentation & imbalance handling as earlier\n",
    "- **Minimum 50 epochs** before early stopping\n",
    "- **Percentage confusion matrix** (each row sums to 100%; diagonal shows per-class accuracy in %)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08226545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ================== Setup & Config ==================\n",
    "import os, random, time, math, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import kagglehub\n",
    "\n",
    "SEED = 42\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "BATCH_SIZE, NUM_WORKERS = 32, 0\n",
    "NUM_EPOCHS, MIN_EPOCHS, PATIENCE = 100, 20, 5\n",
    "LR, WEIGHT_DECAY = 3e-4, 1e-4\n",
    "PRETRAINED, USE_AMP = True, True\n",
    "CLASSES = ['0','1','2','3','4']; N_CLASSES=len(CLASSES); IMG_SIZE=224\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2deca59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits: (4622, 2) (1156, 2) (1656, 2)\n",
      "Class counts: {0: 1829, 1: 837, 2: 1213, 3: 605, 4: 138}\n",
      "Class weights: [0.50541279 1.10442055 0.76207749 1.52793388 6.69855072]\n"
     ]
    }
   ],
   "source": [
    "# ================== Data ==================\n",
    "# path = kagglehub.dataset_download(\"shashwatwork/knee-osteoarthritis-dataset-with-severity\")\n",
    "# print(\"Dataset path:\", path)\n",
    "path = 'Data'\n",
    "\n",
    "DATA_DIR = Path(path); TRAIN_DIR = DATA_DIR/'train'; TEST_DIR = DATA_DIR/'test'\n",
    "\n",
    "def load_dataset_as_dataframe(subdir: str, max_images=None):\n",
    "    root = TRAIN_DIR.parent / subdir; rows=[]\n",
    "    for cls in CLASSES:\n",
    "        cdir = root / cls\n",
    "        if not cdir.exists(): continue\n",
    "        files = sorted([p for p in cdir.iterdir() if p.is_file() and p.suffix.lower() in {'.png','.jpg','.jpeg'}])\n",
    "        if max_images: files = files[:max_images]\n",
    "        for p in files: rows.append({'path': str(p), 'label': int(cls)})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "train_df = load_dataset_as_dataframe('train'); test_df = load_dataset_as_dataframe('test')\n",
    "tr_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['label'], random_state=SEED)\n",
    "print('Splits:', tr_df.shape, val_df.shape, test_df.shape)\n",
    "\n",
    "class_counts = tr_df['label'].value_counts().sort_index(); total = class_counts.sum()\n",
    "class_weights = (total / (len(CLASSES) * class_counts)).to_numpy()\n",
    "sample_weights = tr_df['label'].map({i:w for i,w in enumerate(class_weights)}).to_numpy()\n",
    "print('Class counts:', class_counts.to_dict())\n",
    "print('Class weights:', class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50218810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Transforms & Dataset ==================\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.02),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "eval_tfms = transforms.Compose([\n",
    "    transforms.Resize(int(IMG_SIZE*1.15)),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "class KneeDataset(Dataset):\n",
    "    def __init__(self, df, transform):\n",
    "        self.df = df.reset_index(drop=True); self.transform = transform\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row['path']).convert('RGB')\n",
    "        img = self.transform(img) if self.transform else img\n",
    "        return img, int(row['label'])\n",
    "\n",
    "train_ds = KneeDataset(tr_df, train_tfms)\n",
    "val_ds   = KneeDataset(val_df, eval_tfms)\n",
    "test_ds  = KneeDataset(test_df, eval_tfms)\n",
    "\n",
    "sampler = torch.utils.data.WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a21a7040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== ViT Model ==================\n",
    "def build_vit_b16(num_classes=len(CLASSES), pretrained=True):\n",
    "    vit = models.vit_b_16(weights=models.ViT_B_16_Weights.DEFAULT if pretrained else None)\n",
    "    vit.heads.head = torch.nn.Linear(vit.heads.head.in_features, num_classes)\n",
    "    return vit\n",
    "\n",
    "model = build_vit_b16(pretrained=PRETRAINED).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32, device=device))\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda') and USE_AMP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "769cc08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Train / Eval Utils ==================\n",
    "def accuracy_from_logits(logits, targets):\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    return (preds == targets).float().mean().item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval(); all_logits=[]; all_targets=[]; total=0.0\n",
    "    for imgs,labels in loader:\n",
    "        imgs,labels = imgs.to(device), labels.to(device)\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        total += loss.item()*imgs.size(0)\n",
    "        all_logits.append(logits.detach().cpu()); all_targets.append(labels.detach().cpu())\n",
    "    all_logits = torch.cat(all_logits); all_targets = torch.cat(all_targets)\n",
    "    acc = accuracy_from_logits(all_logits, all_targets)\n",
    "    return total/len(loader.dataset), acc, all_logits, all_targets\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, scaler):\n",
    "    model.train(); total_loss=0.0; total_acc=0.0\n",
    "    for imgs,labels in tqdm(loader, leave=False):\n",
    "        imgs,labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type=='cuda') and USE_AMP):\n",
    "            logits = model(imgs); loss = criterion(logits, labels)\n",
    "        scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
    "        total_loss += loss.item()*imgs.size(0)\n",
    "        total_acc  += accuracy_from_logits(logits.detach(), labels)*imgs.size(0)\n",
    "    n=len(loader.dataset); return total_loss/n, total_acc/n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26f0cc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=1.2206 acc=0.1967 | val_loss=2.1066 acc=0.0303 | lr=3.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | train_loss=1.1700 acc=0.2114 | val_loss=2.1668 acc=0.0303 | lr=3.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | train_loss=1.1923 acc=0.1984 | val_loss=2.1470 acc=0.0303 | lr=3.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00004: reducing learning rate of group 0 to 1.5000e-04.\n",
      "Epoch 04 | train_loss=1.1733 acc=0.1997 | val_loss=2.0330 acc=0.0303 | lr=1.50e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | train_loss=1.1694 acc=0.2042 | val_loss=2.1781 acc=0.0303 | lr=1.50e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06 | train_loss=1.2102 acc=0.1876 | val_loss=2.1157 acc=0.0303 | lr=1.50e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, NUM_EPOCHS+\u001b[32m1\u001b[39m):\n\u001b[32m      6\u001b[39m     t0=time.time()\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     train_loss, train_acc = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     val_loss, val_acc, _, _ = evaluate(model, val_loader)\n\u001b[32m      9\u001b[39m     scheduler.step(val_acc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, optimizer, scaler)\u001b[39m\n\u001b[32m     23\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.cuda.amp.autocast(enabled=(device.type==\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m USE_AMP):\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m; loss = criterion(logits, labels)\n\u001b[32m     26\u001b[39m scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n\u001b[32m     27\u001b[39m total_loss += loss.item()*imgs.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1499\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1500\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1501\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[32m   1503\u001b[39m full_backward_hooks, non_full_backward_hooks = [], []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\venv\\Lib\\site-packages\\torchvision\\models\\vision_transformer.py:298\u001b[39m, in \u001b[36mVisionTransformer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    295\u001b[39m batch_class_token = \u001b[38;5;28mself\u001b[39m.class_token.expand(n, -\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m    296\u001b[39m x = torch.cat([batch_class_token, x], dim=\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[38;5;66;03m# Classifier \"token\" as used by standard language architectures\u001b[39;00m\n\u001b[32m    301\u001b[39m x = x[:, \u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1499\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1500\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1501\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[32m   1503\u001b[39m full_backward_hooks, non_full_backward_hooks = [], []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\venv\\Lib\\site-packages\\torchvision\\models\\vision_transformer.py:157\u001b[39m, in \u001b[36mEncoder.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    155\u001b[39m torch._assert(\u001b[38;5;28minput\u001b[39m.dim() == \u001b[32m3\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected (batch_size, seq_length, hidden_dim) got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    156\u001b[39m \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28minput\u001b[39m + \u001b[38;5;28mself\u001b[39m.pos_embedding\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ln(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1499\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1500\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1501\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[32m   1503\u001b[39m full_backward_hooks, non_full_backward_hooks = [], []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1499\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1500\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1501\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[32m   1503\u001b[39m full_backward_hooks, non_full_backward_hooks = [], []\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ================== Training Loop (Min 50 epochs) ==================\n",
    "best_val_acc=-1.0; history={'train_loss':[], 'train_acc':[], 'val_loss':[], 'val_acc':[], 'lr':[]}\n",
    "ckpt_path='best_vit_b16.pt'; patience_used=0\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    t0=time.time()\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, scaler)\n",
    "    val_loss, val_acc, _, _ = evaluate(model, val_loader)\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "    history['train_loss'].append(train_loss); history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss); history['val_acc'].append(val_acc)\n",
    "    history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
    "          f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | lr={optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({'model_state_dict': model.state_dict(), 'classes': CLASSES}, ckpt_path)\n",
    "        patience_used = 0\n",
    "    else:\n",
    "        patience_used += 1\n",
    "    if epoch >= MIN_EPOCHS and patience_used >= PATIENCE:\n",
    "        print(f\"Early stopping at epoch {epoch}.\"); break\n",
    "print('Best val acc:', best_val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48258249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Curves ==================\n",
    "def plot_curves(history, title='ViT_B_16'):\n",
    "    fig,ax = plt.subplots(figsize=(6,4))\n",
    "    ax.plot(history['train_loss'], label='train_loss')\n",
    "    ax.plot(history['val_loss'], label='val_loss')\n",
    "    ax.set_title(f'{title} — Loss'); ax.set_xlabel('Epoch'); ax.set_ylabel('Loss'); ax.grid(alpha=0.3); ax.legend(); plt.show()\n",
    "\n",
    "    fig,ax = plt.subplots(figsize=(6,4))\n",
    "    ax.plot(history['train_acc'], label='train_acc')\n",
    "    ax.plot(history['val_acc'], label='val_acc')\n",
    "    ax.set_title(f'{title} — Accuracy'); ax.set_xlabel('Epoch'); ax.set_ylabel('Accuracy'); ax.grid(alpha=0.3); ax.legend(); plt.show()\n",
    "\n",
    "plot_curves(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c9a4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Percentage Confusion Matrix ==================\n",
    "@torch.no_grad()\n",
    "def percent_confusion_matrix(logits, targets, classes=CLASSES):\n",
    "    y_true = targets.numpy()\n",
    "    y_pred = torch.argmax(logits,1).numpy()\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes)))).astype(np.float64)\n",
    "    row_sums = cm.sum(axis=1, keepdims=True); row_sums[row_sums==0]=1.0\n",
    "    cm_pct = (cm/row_sums)*100.0\n",
    "    df = pd.DataFrame(np.round(cm_pct,2), index=[f'True {c}' for c in classes],\n",
    "                      columns=[f'Pred {c}' for c in classes])\n",
    "    diag = np.diag(cm_pct); support = cm.sum(axis=1).astype(int)\n",
    "    summary = pd.DataFrame({'Class': classes, 'Per-Class Acc (%)': np.round(diag,2), 'Support': support})\n",
    "    return df, summary\n",
    "\n",
    "ckpt = torch.load('best_vit_b16.pt', map_location=device)\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "\n",
    "val_loss, val_acc, val_logits, val_targets = evaluate(model, val_loader)\n",
    "test_loss, test_acc, test_logits, test_targets = evaluate(model, test_loader)\n",
    "\n",
    "print(f\"VAL  loss={val_loss:.4f} acc={val_acc:.4f}\")\n",
    "val_df, val_summary = percent_confusion_matrix(val_logits, val_targets); display(val_df); display(val_summary)\n",
    "\n",
    "print(f\"TEST loss={test_loss:.4f} acc={test_acc:.4f}\")\n",
    "test_df, test_summary = percent_confusion_matrix(test_logits, test_targets); display(test_df); display(test_summary)\n",
    "\n",
    "val_df.to_csv('vit_val_percent_confusion.csv'); test_df.to_csv('vit_test_percent_confusion.csv')\n",
    "val_summary.to_csv('vit_val_per_class_accuracy.csv', index=False)\n",
    "test_summary.to_csv('vit_test_per_class_accuracy.csv', index=False)\n",
    "print('Saved CSVs.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
